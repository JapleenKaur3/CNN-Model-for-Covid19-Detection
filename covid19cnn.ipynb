{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import system libs\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "# import data handling tools\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# import data for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import load_img, array_to_img, img_to_array\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print ('modules loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    classes = os.listdir(data_dir)\n",
    "    images = []\n",
    "    masks = []\n",
    "    labels = []\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            image_subdir = os.path.join(class_dir, 'images')\n",
    "            mask_subdir = os.path.join(class_dir, 'masks')\n",
    "\n",
    "            for filename in os.listdir(image_subdir):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    # Load image\n",
    "                    image_path = os.path.join(image_subdir, filename)\n",
    "                    if not os.path.exists(image_path):\n",
    "                        print(f\"Image not found: {image_path}\")\n",
    "                        continue\n",
    "\n",
    "                    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    image = cv2.resize(image, (128,128))\n",
    "                    images.append(image)\n",
    "\n",
    "                    # Load mask\n",
    "                    mask_filename = filename\n",
    "                    mask_path = os.path.join(mask_subdir, mask_filename)\n",
    "                    if not os.path.exists(mask_path):\n",
    "                        print(f\"Mask not found: {mask_path}\")\n",
    "                        continue\n",
    "\n",
    "                    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    mask = cv2.resize(mask, (128,128))\n",
    "                    masks.append(mask)\n",
    "\n",
    "                    # Assign label based on class name\n",
    "                    labels.append(class_name)\n",
    "\n",
    "    return np.array(images), np.array(masks), np.array(labels)\n",
    "\n",
    "data_dir = \"COVID-19_Radiography_Dataset\"\n",
    "images, masks, labels = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(images))\n",
    "print(len(masks))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_with_masks(images, masks, labels, num_images=10):\n",
    "    # Select a random subset of indices\n",
    "    indices = np.random.choice(len(images), num_images, replace=False)\n",
    "\n",
    "    plt.figure(figsize=(15, 6 * num_images // 5))\n",
    "\n",
    "    for i, idx in enumerate(indices, start=1):\n",
    "        #Images\n",
    "        plt.subplot(num_images, 6, 6 * i - 2)\n",
    "        plt.imshow(images[idx])\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Image - Label: {labels[idx]}\")\n",
    "\n",
    "        #Masks\n",
    "        plt.subplot(num_images, 6, 6 * i - 1)\n",
    "        plt.imshow(masks[idx], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Mask\")\n",
    "\n",
    "        # Overlay mask on the image\n",
    "        plt.subplot(num_images, 6, 6 * i)\n",
    "        plt.imshow(images[idx])\n",
    "        plt.imshow(masks[idx], cmap='jet', alpha=0.5)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Image with Mask\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_images_with_masks(images, masks, labels, num_images=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_masks, test_masks = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "train_images = np.array(train_images).reshape(len(train_images),128,128)\n",
    "test_images = np.array(test_images).reshape(len(test_images),128,128)\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "print(train_masks.shape)\n",
    "print(test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(shape, out_ch, rate=1):\n",
    "    x = tf.keras.layers.Conv2D(out_ch, 3, padding=\"same\", dilation_rate=1)(shape)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def RSU_L(shape, out_ch, M_ch, num_layers, rate=2):\n",
    "    x = conv_block(shape, out_ch)\n",
    "    inp_ch = x\n",
    "\n",
    "    skip_features = []\n",
    "    x = conv_block(x, M_ch)\n",
    "    skip_features.append(x)\n",
    "\n",
    "    for i in range(num_layers-2):\n",
    "        x = tf.keras.layers.MaxPool2D((2, 2))(x)\n",
    "        x = conv_block(x, M_ch)\n",
    "        skip_features.append(x)\n",
    "\n",
    "    x = conv_block(x, M_ch, rate=rate)\n",
    "    skip_features.reverse()\n",
    "    x = tf.keras.layers.Concatenate()([x, skip_features[0]])\n",
    "    x = conv_block(x, M_ch)\n",
    "\n",
    "    for i in range(num_layers-3):\n",
    "        x = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip_features[i+1]])\n",
    "        x = conv_block(x, M_ch)\n",
    "\n",
    "    x = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip_features[-1]])\n",
    "    x = conv_block(x, out_ch)\n",
    "\n",
    "    x = tf.keras.layers.Add()([x, inp_ch])\n",
    "    return x\n",
    "\n",
    "def RSU_4F(shape, out_ch, M_ch):\n",
    "    x0 = conv_block(shape, out_ch, rate=1)\n",
    "\n",
    "    x1 = conv_block(x0, M_ch, rate=1)\n",
    "    x2 = conv_block(x1, M_ch, rate=2)\n",
    "    x3 = conv_block(x2, M_ch, rate=4)\n",
    "\n",
    "    x4 = conv_block(x3, M_ch, rate=8)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([x4, x3])\n",
    "    x = conv_block(x, M_ch, rate=4)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([x, x2])\n",
    "    x = conv_block(x, M_ch, rate=2)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([x, x1])\n",
    "    x = conv_block(x, out_ch, rate=1)\n",
    "\n",
    "    x = tf.keras.layers.Add()([x, x0])\n",
    "    return x\n",
    "\n",
    "def u2net(shape, out_ch, M_ch, num_classes=1):\n",
    "    inputs = tf.keras.layers.Input(shape)\n",
    "\n",
    "    e1 = RSU_L(inputs, out_ch[0], M_ch[0], 7)\n",
    "    p1 = tf.keras.layers.MaxPool2D((2, 2))(e1)\n",
    "\n",
    "    e2 = RSU_L(p1, out_ch[1], M_ch[1], 6)\n",
    "    p2 = tf.keras.layers.MaxPool2D((2, 2))(e2)\n",
    "\n",
    "    e3 = RSU_L(p2, out_ch[2], M_ch[2], 5)\n",
    "    p3 = tf.keras.layers.MaxPool2D((2, 2))(e3)\n",
    "\n",
    "    e4 = RSU_L(p3, out_ch[3], M_ch[3], 4)\n",
    "    p4 = tf.keras.layers.MaxPool2D((2, 2))(e4)\n",
    "\n",
    "    e5 = RSU_4F(p4, out_ch[4], M_ch[4])\n",
    "    p5 = tf.keras.layers.MaxPool2D((2, 2))(e5)\n",
    "\n",
    "    b1 = RSU_4F(p5, out_ch[5], M_ch[5])\n",
    "    b2 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(b1)\n",
    "\n",
    "    d1 = tf.keras.layers.Concatenate()([b2, e5])\n",
    "    d1 = RSU_4F(d1, out_ch[6], M_ch[6])\n",
    "    u1 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(d1)\n",
    "\n",
    "    d2 = tf.keras.layers.Concatenate()([u1, e4])\n",
    "    d2 = RSU_L(d2, out_ch[7], M_ch[7], 4)\n",
    "    u2 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(d2)\n",
    "\n",
    "    d3 = tf.keras.layers.Concatenate()([u2, e3])\n",
    "    d3 = RSU_L(d3, out_ch[8], M_ch[8], 5)\n",
    "    u3 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(d3)\n",
    "\n",
    "    d4 = tf.keras.layers.Concatenate()([u3, e2])\n",
    "    d4 = RSU_L(d4, out_ch[9], M_ch[9], 6)\n",
    "    u4 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(d4)\n",
    "\n",
    "    d5 = tf.keras.layers.Concatenate()([u4, e1])\n",
    "    d5 = RSU_L(d5, out_ch[10], M_ch[10], 7)\n",
    "\n",
    "\n",
    "    y1 = tf.keras.layers.Conv2D(num_classes, 3, padding=\"same\")(d5)\n",
    "\n",
    "    y2 = tf.keras.layers.Conv2D(num_classes, 3, padding=\"same\")(d4)\n",
    "    y2 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(y2)\n",
    "\n",
    "    y3 = tf.keras.layers.Conv2D(num_classes, 3, padding=\"same\")(d3)\n",
    "    y3 = tf.keras.layers.UpSampling2D(size=(4, 4), interpolation=\"bilinear\")(y3)\n",
    "\n",
    "    y4 = tf.keras.layers.Conv2D(num_classes, 3, padding=\"same\")(d2)\n",
    "    y4 = tf.keras.layers.UpSampling2D(size=(8, 8), interpolation=\"bilinear\")(y4)\n",
    "\n",
    "    y5 = tf.keras.layers.Conv2D(num_classes, 3, padding=\"same\")(d1)\n",
    "    y5 = tf.keras.layers.UpSampling2D(size=(16, 16), interpolation=\"bilinear\")(y5)\n",
    "\n",
    "    y6 = tf.keras.layers.Conv2D(num_classes, 3, padding=\"same\")(b1)\n",
    "    y6 = tf.keras.layers.UpSampling2D(size=(32, 32), interpolation=\"bilinear\")(y6)\n",
    "\n",
    "    y0 = tf.keras.layers.Concatenate()([y1, y2, y3, y4, y5, y6])\n",
    "    y0 = tf.keras.layers.Conv2D(num_classes, 3, padding=\"same\")(y0)\n",
    "\n",
    "    y0 = tf.keras.layers.Activation(\"sigmoid\")(y0)\n",
    "    y1 = tf.keras.layers.Activation(\"sigmoid\")(y1)\n",
    "    y2 = tf.keras.layers.Activation(\"sigmoid\")(y2)\n",
    "    y3 = tf.keras.layers.Activation(\"sigmoid\")(y3)\n",
    "    y4 = tf.keras.layers.Activation(\"sigmoid\")(y4)\n",
    "    y5 = tf.keras.layers.Activation(\"sigmoid\")(y5)\n",
    "    y6 = tf.keras.layers.Activation(\"sigmoid\")(y6)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs, outputs=y0, name=\"U2-Net\")\n",
    "    return model\n",
    "\n",
    "def build_u2net(input_shape, num_classes=1):\n",
    "    out_ch = [64, 128, 256, 512, 512, 512, 512, 256, 128, 64, 64]\n",
    "    M_ch = [32, 32, 64, 128, 256, 256, 256, 128, 64, 32, 16]\n",
    "    model = u2net(input_shape, out_ch, M_ch, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "def build_u2net_lite(input_shape, num_classes=1):\n",
    "    out_ch = [64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n",
    "    M_ch = [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]\n",
    "    input_shape_with_channels = input_shape + (1,)\n",
    "    model = u2net(input_shape_with_channels, out_ch, M_ch, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "u2net_model = build_u2net_lite((128,128))\n",
    "#u2net_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=1e-3)\n",
    "u2net_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "    checkpoint = ModelCheckpoint('ChestSegmentor.hdf5',verbose=1, save_best_only= True)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "    return [reduce_lr, checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsitory = u2net_model.fit(train_images, train_masks, validation_split = 0.2, batch_size = 16, epochs = 100, callbacks=get_callbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2net_model.load_weights(\"ChestSegmentor.hdf5\")\n",
    "def generate_masks(model, test_images):\n",
    "    masks = model.predict(test_images)\n",
    "    return masks\n",
    "\n",
    "def plot_testimages_with_masks(images, masks, num_images=10):\n",
    "    # Select a random subset of indices\n",
    "    indices = np.random.choice(len(images), num_images, replace=False)\n",
    "\n",
    "    plt.figure(figsize = (15, 6 * num_images // 5))\n",
    "\n",
    "    for i, idx in enumerate(indices, start=1):\n",
    "        #Images\n",
    "        plt.subplot(num_images, 6, 6 * i - 2)\n",
    "        plt.imshow(images[idx])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Image\")\n",
    "\n",
    "        #Masks\n",
    "        plt.subplot(num_images, 6, 6 * i - 1)\n",
    "        plt.imshow(masks[idx], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Mask\")\n",
    "\n",
    "        # Overlay mask on the image\n",
    "        plt.subplot(num_images, 6, 6 * i)\n",
    "        plt.imshow(images[idx])\n",
    "        plt.imshow(masks[idx], cmap='jet', alpha=0.5)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Image with Mask\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generated_masks = generate_masks(u2net_model, test_images)\n",
    "plot_testimages_with_masks(test_images, generated_masks, num_images=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(labels)\n",
    "encoded_labels = to_categorical(encoded_labels, num_classes=4)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images, encoded_labels, test_size=0.2, random_state=42)\n",
    "train_images = np.array(train_images).reshape(len(train_images),128,128)\n",
    "test_images = np.array(test_images).reshape(len(test_images),128,128)\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (existing code)\n",
    "\n",
    "def build_classification_model_with_regularization(base_model, dropout_rate=0.5):\n",
    "    u2net_base = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "    for layer in u2net_base.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    flat_layer = Flatten()(u2net_base.output)\n",
    "    dropout_layer = Dropout(dropout_rate)(flat_layer)  # Add dropout for regularization\n",
    "    dense_layer = Dense(256, activation='relu')(dropout_layer)\n",
    "    output_layer = Dense(4, activation='softmax')(dense_layer)\n",
    "\n",
    "    classification_model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "    return classification_model\n",
    "\n",
    "# Adjust hyperparameters\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "# Build and compile the classification model with regularization\n",
    "classification_model = build_classification_model_with_regularization(u2net_model, dropout_rate=0.5)\n",
    "opt = Adam(learning_rate=learning_rate)\n",
    "classification_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Implement learning rate scheduling\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 20:\n",
    "        return learning_rate\n",
    "    else:\n",
    "        return learning_rate * tf.math.exp(0.1 * (20 - epoch))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Save the best model weights during training\n",
    "checkpoint_filepath = 'classification_model.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# Train the model with regularization and learning rate scheduling\n",
    "history = classification_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[model_checkpoint_callback, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_result = classification_model.evaluate(test_images, test_labels)\n",
    "\n",
    "accuracy = evaluation_result[1]\n",
    "print(f'Model Accuracy on Test Data: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
